# 吴恩达（斯坦福）机器学习笔记

[toc]

[吴恩达-机器学习（Coursera）](https://www.coursera.org/learn/machine-learning)

## 1 概述

### 1.1 定义

*Arthur Samuel*

“Field of study that gives computers the ability to learn without being explicitly programmed.”

> 机器学习：在没有进行特定编程的情况下给予计算机学习能力的领域。
>



*Tom Mitchell*

*“A computer program is said to *learn* from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.”*

> 机器学习：机器学习是为了执行**任务T**，从**经验E**中学习，用**表现P**来评判它完成任务的性能，并从中提升。
>



**举例**

- 过滤垃圾邮件

用Tom的定义来说，T=正确分类垃圾邮件和普通邮件，E=查看你对邮件的人工标记（是否是垃圾邮件），P=正确分类的数量或概率。



### 1.2 学习算法

#### 1.2.1 监督学习

监督学习：提供“正确答案”的数据集，来**教**计算机如何学习

##### 1.2.1.1 回归问题

回归问题：预测出连续值属性的类型

**举例**

- 房价预测

根据已知的**面积-房价**数据，得到一条较为拟合的函数（图中的紫色非线性函数，蓝色线性函数），通过回归预测出X面积房子的房价Y。

![房价预测](https://s2.ax1x.com/2020/01/02/lt0lw9.png)





##### 1.2.1.2 分类问题

分类问题：预测出离散值属性的类型

**举例**

- 肿瘤预测

根据已知的**年龄;肿瘤大小-良性/恶性**数据，得到一条拟合直线来区分良性和恶性，从而通过年龄和肿瘤大小来判断肿瘤是良性或恶性的（蓝圈良性，红叉恶性）。

![肿瘤预测](https://s2.ax1x.com/2020/01/02/lt08F1.png)



通过例子，我们了解到，我们需要通过事物的许多属性（房子面积、肿瘤大小和患者年龄等）来得到我们需要的结果（房价、良性或恶性）。属性是影响结果的一些关键性因素，而在实际中，我们影响事物的属性非常多，可计算机存储空间有限，不可能存放无限多的属性，所以我们要用到 **支持向量机** 来解决这个问题。支持向量机通过一个简洁的数学技巧，允许计算基础护理无限多的属性。



#### 1.2.2 非监督学习

非监督学习：没有正确答案的数据集，让计算机从中自主找到某种蕴含的结构

##### 1.2.2.1 聚类算法

聚类算法：找出数据集中“蕴含”的分类

**举例**

- 肿瘤分类

![肿瘤分类](https://s2.ax1x.com/2020/01/02/lt0GJx.png)

- 新闻分类

![image-20200102182137114](https://s2.ax1x.com/2020/01/02/lt01oR.png)





**鸡尾酒会算法**

通过Microphone #1和Microphone #2的声音，分离仅有出Speaker #1和Speaker #2的声音

![鸡尾酒会算法](https://s2.ax1x.com/2020/01/02/ltscNj.png)

```matlab
[W,s,v]=svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');
# svd:奇异值分解
```



---

## 2 单特征线性回归

### 2.1 定义

m=训练集样本数量

x=输入变量/特征

y=输出变量

h=映射函数(prediction)

轮廓图(contour plot)

![轮廓图](https://s2.ax1x.com/2020/01/07/l6dFgg.png)

1. 相同颜色的每一圈表示$J(\theta_0,\theta_1)$值相等点集合
2. $J(\theta_0,\theta_1)$最小值的点在圈最小的点集中

### 2.2 假设函数

$h_θ(x)=θ₀+θ₁x$

![线性回归](https://s2.ax1x.com/2020/01/07/l6lXo4.png)

损失函数选择思路：选择一个函数$J(θ_0,θ_1)$，使得样本$y$与$h_θ(x)$之差最小。



**假设函数和损失函数的区别：**

假设函数是输入值和输出值的函数，是我们解决问题最终需要得到的函数，我们应该尽量使其与样本数据拟合。

损失函数是为了求解最优假设函数而设的函数，其目的是得到假设函数中的各个常量值。损失函数衡量了假设函数与实际问题之间的接近程度。



### 2.3 损失函数

#### 2.3.1 平方差函数

平方差函数是基于：“离差平方和”，它反映了x与其数学期望Ex的偏离程度。

**Prediction:** $h_\theta(x)=\theta_0+\theta_1x$

**Parameters:** $\theta_0,\theta_1$

**Cost Function: **$J(\theta_0,\theta_1)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$ 

**Goal:** $J(\theta_0,\theta_1)min$

> 问：此处本来乘以1/m即可，为什么要乘以1/2m呢？
>
> 答：因为我们需要求$J_{min}$，利用梯度下降法对J求导时，如果是1/2m，求导结果为：，$\frac{∂J}{∂ {\theta}_i}=\frac{1}{m} \sum ^m _{i=1} (h_\theta(x^{(i)})-y^{(i)}) \frac{∂h_{\theta}(x^{(i)})}{∂\theta}$正好求导时平方的2可以消去，简化了计算。
>
> 这里无论除以2m还是m，函数J最小值的θ都是相同的。

![平方差函数](https://s2.ax1x.com/2020/01/07/l6YfoT.png)



### 2.4 损失函数最小化

#### 2.4.1 梯度下降法

梯度下降法的计算过程就是沿梯度下降/上升的方向求解极小值/极大值（局部/全局最优解）。

用于线性回归的损失函数都是“弓型”的，即凸函数，不存在局部最优解，一定存在并且仅有一个全局最优解。	



**求解步骤：**

1. 选择一个损失函数$J(\theta_0,\theta_1 )$
2. 初始化：$(\theta_0,\theta_1)=(0,0)$
3. 选择一个学习速率$\alpha$ (即梯度下降的速度)  
4. 开始迭代，令$\theta_j:=\theta_j-\alpha \frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)$
5. 检查$\theta_j$是否趋于收敛（$\frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1) \to 0$ 或 $\theta_j \to \theta_{j-1}$），若收敛则得到$(\theta_0,\theta_1)$，反之重复步骤3

![梯度下降求解](https://s2.ax1x.com/2020/01/07/l6DnL4.png)



**关于初始值：**

1. 若损失函数存在多个局部最优解，则从不同的初始值进行梯度下降，可能得到不同的局部最优解。

![局部最优解](https://s2.ax1x.com/2020/01/07/l6yc6K.png)



**关于偏导：**

1. $\theta_0:=\theta_0-\alpha \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)}) $
2. $\theta_1:=\theta_1-\alpha \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})\cdot x^{(i)}$ 



**关于学习速率$\alpha$ ：**

1. $\alpha$如果过大，可能导致无法收敛；过小，可能导致求解速度太慢。
2. $\alpha$不需要在循环中改变大小的原因：因为越接近min，导数越接近0，所以导×$\alpha$也会趋于0，故梯度下降算法会自动采取更小的下降步法。

![梯度下降步法自动减小](https://s2.ax1x.com/2020/01/07/l6gIh9.png)



**梯度下降的三种形式：**

该三种形式的损失函数不同

- 批量梯度下降（Batch Gradient Descent，BGD）

$$J(\theta_0,\theta_1)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$$

```伪代码
repeat{
    θj:=θj−α1m∑mi=1(hθ(x(i))−y(i))x(i)j
	(for j =0,1)
}
```



- 随机梯度下降（Stochastic Gradient Descent，SGD）

$$J^{(i)}(\theta_0,\theta_1)=\frac{1}{2} (h_θ(x^{(i)})-y^{(i)})^2$$

```伪代码
repeat{
	for i=1,...,m{
		θj:=θj−α(hθ(x(i))−y(i))x(i)j
		(for j =0,1)
	}
}
```



- 小批量梯度下降（Mini-Batch Gradient Descent, MBGD）

$$J(\theta_0,\theta_1)=\frac{1}{2m}  \sum ^k _{i=1} (h_θ(x^{(i)})-y^{(i)})^2 ,k\in(0,m)$$

```伪代码
repeat{
	for i=1,11,21,31,...,991{
		θj:=θj−α110∑(i+9)k=i(hθ(x(k))−y(k))x(k)j
		(for j =0,1)
	}
}
```



参考：

[三种梯度下降形式的介绍](https://www.cnblogs.com/lliuye/p/9451903.html)

[梯度下降理解](https://www.jianshu.com/p/c7e642877b0e)



### 2.5 练习

1. 根据分子中碳含量和燃烧释放能量的训练集，构建一个线性回归函数

![题目](https://s2.ax1x.com/2020/01/07/lcyME4.png)

**问题解决思路**

1. 根据条件划出点集图，点集分布符合线性回归问题，故**确认该问题类型**
2. 线性回归问题**构建假设函数**：$h_\theta(x)=\theta_0+\theta_1*x$ 
3. 明确所需求参为 $\theta_0,\theta_1$ ，**构建损失函数**，此处采用平方差函数：$J(\theta_0,\theta_1)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$ 
4. **明确最小化损失函数方法**，此处采用批量梯度下降法：
   1. 初始化：$(\theta_0,\theta_1)=(0,0)$
   2. 选择一个学习速率$\alpha$ （常以0.001开始递增尝试）
   3. 选择一个迭代次数iteraNum（常以1500开始）
   4. 迭代过程中令$\theta_j:=\theta_j-\alpha \frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)$

Octave实现代码如下：

```matlab
data = load('trainingset.txt');
X = data(:,1);
y = data(:,2);

%定义梯度下降算法
function theta = gradientDescent(X, y, alpha, iteraNum)
theta = [0;0];
m = length(y);
for i=1:iteraNum;
dist = theta(1) + X*theta(2) - y;
theta(1) = theta(1) - alpha*(1/m)*sum(dist);
theta(2) = theta(2) - alpha*(1/m)*sum(dist.*X);
end
end

%默认学习速率为0.01 迭代次数为5000次
theta = gradientDescent(X, y, 0.01, 5000);
plot(X, y, 'xr', X, theta(1) + theta(2)*X);
legend('Training Data','Linear Regression');
xlabel('Number of hydrocarbons in molecule');
ylabel('Heat release when burned(kJ/mol)');

%theta = [-569,60; -530,91]
```

训练后得到的回归函数为 $h_\theta(x)=-569.6 - 530.9*x$ :

![效果图](https://s2.ax1x.com/2020/01/07/lcgQUg.png)



**矩阵在该算法中的应用：**

1. 对于训练集$(x_0,x_1,x_2,...,x_n,y)$，将$(x_0,x_1,x_2,...,x_n)$作为矩阵$X$，$(y)$作为矩阵(向量) $y$
2. 对损失函数的偏导$\frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)$ 包含 $\sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})$ ，为保证**同时更新**的要求，应先用矩阵乘法表示为：$dist=h_\theta(x^{(i)})-y^{(i)}= \theta_0 + X\cdot\theta_1 - y$
3. $\theta_1= \theta_1 - \alpha*(1/m)*sum(dist)$
4. $\theta_2= \theta_2 - \alpha*(1/m)*sum(dist.*X)$ ，因为$J(\theta_1,\theta_2)$的两个偏导不同，对$\theta_2$的偏导需要再乘以$x$，故此处需要采用$dist.*X$，其中的“ $.*$ ”是指同型矩阵$a_{ij}*b_{ij}$ 



## 3 多特征线性回归

### 3.1 定义

$m$=样本数量

$n$ = 特征数量

$x^{(i)}$=第i个样本

$x^{(i)}_j$=第i个样本的第j个特征



### 3.2 假设函数

$$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n$$ 

1. 为了方便，令 $x_0=1$，即 $x^{(i)}_0=1$ 
2. 令 $X= \begin{bmatrix} x_0 \ x_1 \ x_2 \ \dots \ x_n \end{bmatrix} $ ， $\theta=\left[\begin{matrix} \theta_0\\\theta_1\\\theta_2\\\vdots\\\theta_n \end{matrix}\right]$ ，则 $h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n=X\theta$



**Code**

依据公式：$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n=X\theta$

```matlab
prediciton = X*theta;
```



### 3.3 损失函数

#### 3.3.1 平方差函数

平方差函数是基于：“离差平方和”，它反映了x与其数学期望Ex的偏离程度。

**Prediction:** $h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n =  X\theta$

**Parameters:** $\theta_0,\theta_1,\cdots,\theta_n$ 也就是 $\theta^T=[\theta_0  \theta_1 \cdots \theta_n]$

**Cost Function: **$J(\theta)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$ 

**Goal:** $J(\theta)min$



**Code**

依据公式：$J(\theta)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$ 

```matlab
sqrErrors = (X*theta-y).^2;		%squared errors
J = 1/(2*m) * sum(sqrErrors);
```



### 3.4 损失函数最小化

#### 3.4.1 梯度下降法

**求解步骤（完善）：**

1. 选择一个损失函数$J(\theta)$ 
2. 初始化：$\theta=0$ 
3. 特征的缩放以及均值归一化
4. 选择一个学习速率$\alpha$ ，画出 $J(\theta)$ — $iteraNum$ 的图像，找出最合适的学习速率
5. 开始迭代，令$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)$
6. 检查$\theta_j$是否趋于收敛（$\frac{\partial}{\partial \theta_j} J(\theta) \to 0$ 或 $\theta^j \to \theta^{j-1}$），若收敛则得到 $\theta$，反之重复步骤3



**关于偏导：**

1. $\theta_0:=\theta_0-\alpha \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)}) $
2. $\theta_j:=\theta_j-\alpha \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})\cdot x^{(i)}_j$ 



**Code**

依据公式：$\theta_j:=\theta_j-\alpha \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})\cdot x^{(i)}_j$ 

```matlab
for iter = 1:num_iters
	% X补充了一列1，所以可以直接乘X'
    theta = theta - (alpha/m)*X'*(X*theta - y);
end
```



#### 3.4.2 正规方程法

采用数学中求解多项式函数的方法：

1. 求每个参数的偏导
2. 偏导置零得极值点
3. 极值点中选择出最小值点

基本思想如下：

![θ的举证表示](https://s2.ax1x.com/2020/01/19/1CdGM4.png)



**Code**

依据公式： $\theta=(X^TX)^{-1}X^Ty$

```matlab
theta = pinv(X' * X) * X' * y;			%pinv()返回矩阵伪逆阵
```



对于极少数不可逆的矩阵$X^TX$，我们仍然用`pinv()`函数。因为该函数是“伪逆”的，他在运算过程中与真正的求逆函数`inv()`不同，会在一些步骤中有一点变动，但不影响结果。



矩阵不可逆的两个原因：

1. 当矩阵的两个特征选取不恰当时，可能会导致$X^TX$不可逆的情况出现。例如特征”英尺“和”米“作为两个特征，但实际上两个特征线性相关，则矩阵一定会不可逆。
2. 样本数<特征数时，可能会出现不可逆情况。此时就需要采用正则化等方法，使较少的样本仍然能得到有很多特征的参数。

矩阵不可逆时，应先尝试排除冗余的特征。



#### 3.4.3 方法比较

| 梯度下降法                            | 正规方程法                                             |
| ------------------------------------- | ------------------------------------------------------ |
| 需要通过多次调试，找到合适的学习速率α | 不需要学习速率α                                        |
| 需要多次迭代                          | 不需要迭代                                             |
| 特征的数量对算法速度影响不大          | θ包含$(X^TX)^{-1}$，时间复杂度$O(n^3)$，故特征不宜太多 |
|                                       | 经验：特征数量>1万时，速度将会有较大影响               |



### 3.5 特征选择

对于多个特征，例如房子的宽度和深度，可以设面积=宽度*深度，从而将特征减少。也即提醒，特征的选择应该遵循该特征是对结果有影响的。

#### 3.5.1 多项式回归

多项式回归使得我们可以采取线性回归的方法去拟合非常复杂甚至是非线性的函数。

如下图，将一元非线性多项式函数的n次方变量(n>1)，作为一个新特征值，从而变换成多元线性多项式函数。但需要注意的是，此时需要对n次方的变量进行特征缩放操作。

![多项式替代非线性函数](https://s2.ax1x.com/2020/01/09/lWlTbt.png)



### 3.6 特征处理



#### 3.5.1 归一化

若多特征值问题中，不同特征值在相近范围中时，采用特征缩放可使梯度下降更快地收敛。

$x_i=\frac{x_i}{s_i}$  其中$s_i$是$x_i$的取值范围大小$|max-min|$

下图中，针对“房屋面积”和“房间数量”两个特征，其相去甚远，轮廓图如左侧所示，采用梯度下降时，往往需要很长时间才能曲折找到收敛的值。当我们将特征值的取值范围缩放到[0,1]中时，轮廓图呈近似圆形，故此时进行梯度下降，将会更加快捷。

![特征缩放示意](https://s2.ax1x.com/2020/01/08/l2AuDJ.png)

实际中常缩放到[-1,1]中，但并非要求每个特征值都要缩放到该范围，只要在[-1,1] 的附近均可，例如[0,3] [-2,0.5]，本质只是将特征值范围的相差减小。同理，过小也需要放大。目前经验建议的范围是[-3,3] 至 [-1/3,1/3] 都无需缩放。



**均值归一化：**

将特征值的范围尽量关于0对称

$x_i=\frac{x_i-\mu_i}{s_i}$  其中$\mu_i$是$x_i$的平均值$\frac{\sum ^n _{i=1} {x_i}}{n}$



#### 3.4.2 标准化

将特征值转变为平均数为0，标准差为1的集合

$x_i=\frac{x_i-\mu_i}{\sigma}$ 其中$\sigma$是该特征集合的标准差



> 归一化和标准化的区别：
>
> 归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。



#### 3.4.3 调试

**梯度下降正常工作的标准：** $J(\theta)$随着迭代次数的增加，单调递减，且趋于平缓。

画出 $J(\theta)$ — $iteraNum$ 的图像，下图为正常工作的图像。

![正常工作的Gradient-Descent](https://s2.ax1x.com/2020/01/09/lWuayq.png)

下图为学习速率$\alpha$不合适导致没有正常工作的图像。

![非正常工作的Gradient-Descent](https://s2.ax1x.com/2020/01/09/lWuWOx.png)

1. 如果$\alpha$太小，则收敛速度太慢。
2. 如果$\alpha$太大，则可能无法迭代出最小值。

故学习速率$\alpha$可以通过上述 $J(\theta)$ — $iteraNum$ 图，来判断梯度下降算法是否能正常进行。





### 3.7 练习

1. 根据数据集中，房子面积、房间数，判断房子出售价格

**总流程**

```matlab
%% ========== 读取数据 ==========
fprintf('\nLoading dataset...\n');
data = load('dataset.txt');
n = size(data,2);						%特征+结果数（列数）
m = size(data,1);						%样本数（行数）
X = data(:,1:n-1);					%多特征的属性值矩阵X
y = data(:,n);						%结果向量

%% ========== 查看散点图（单特征） ==========
%{
fprintf('\nPlotting Data...\n');
figure;
plot(x, y, 'rx', 'MarkerSize', 10);			%大小为10的红色十字散点
xlabel('description of x-axis');
ylabel('description of y-axis');
kbhit();							%暂停
%}

%% ========== 特征标准化 ==========
[X mu sigma] = featureNormalize(X);			%得到标准化的X以及均值和标准差

%% ========== 初始化 ==========
fprintf('\nInitialing Variables...\n');
X = [ones(m, 1), X];					%补充一列常数1
theta = zeros(n, 1);					%初始化theta为n维向量
iterations = 500;						%迭代次数
alpha = 0.01;							%学习速率

%% ========== 损失函数测试（可选） ==========
%{
fprintf('\nTesting the cost function ...\n');
J = computeCost(X, y, theta);			%测试损失函数是否正常
fprintf('With theta = [0 ; 0]\nCost computed = %f\n', J);
kbhit();
%}

%% ========== 梯度下降法 ==========
[theta, J_history] = gradientDescent(X, y, theta, alpha, iterations);
kbhit();

%% ========== 正规方程法 ==========
function [theta] = normalEqn(X, y)

%% ========== 损失函数调试（单特征） ==========
%{
checkJtheta(X, y, theta);
%}

%% ========== 线性回归预测 ==========
%梯度下降法，需要对参数进行相应的特征处理
x_pre = [1650 3];
x_pre_norm = (x_pre - mu)./sigma;							%特征标准化
x_pre_norm = [ones(size(x_pre_norm,1),1),x_pre_norm];		%补充1列
price = x_pre_norm * theta;

%正规方程法直接代入参数即可
x_pre = [1650 3];
x_pre = [ones(size(x_pre,1),1),x_pre];
price = x_pre * theta;
```



**损失函数**

依据公式：$J(\theta)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$ 

```matlab
%该损失函数为平方差函数
function J = computeCost(X, y, theta)

% X is the "design matrix" containing our training examples.
% y is the class labels.
m = length(y);				% number of training examples
sqrErrors = (X*theta-y).^2;		%squared errors
J = 1/(2*m) * sum(sqrErrors);

end
```



**特征标准化函数**

依据公式：$x_i=\frac{x_i-\mu_i}{\sigma}$

```matlab
function [X_norm, mu, sigma] = featureNormalize(X)

X_norm = X;
mu = zeros(1, size(X, 2));				%均值向量初始化
sigma = zeros(1, size(X, 2));			%标准差向量初始化

mu = mean(X);    						%计算每个特征的均值
sigma = std(X);    						%计算每个特征的标准差
X_norm = (X - mu) ./ sigma;    			%计算标准化后的特征矩阵

end
```



**梯度下降函数**

依据公式：$\theta_j:=\theta_j-\alpha \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})\cdot x^{(i)}_j$ 

```matlab
function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters
    theta = theta - (alpha/m)*X'*(X*theta - y);
    J_history(iter) = computeCost(X, y, theta);
end

hold on;								%在离散图中画线
plot(X(:,2:size(X,2)), X*theta, '-');			%X的2~n列用-线画出
legend('Training data','Linear regression');
hold off;

end
```



**正规方程函数**

依据公式： $\theta=(X^TX)^{-1}X^Ty$

```matlab
function [theta] = normalEqn(X, y)

theta = zeros(size(X, 2), 1);
theta = pinv(X' * X) * X' * y;			%pinv()返回矩阵伪逆阵

end
```



**3D函数&轮廓图判断梯度下降性能函数（单特征）：**

```matlab
%判断损失函数是否正常工作
function checkJtheta(X, y, theta)

fprintf('\nVisualizing J(theta_0, theta_1) ...\n');

theta0_vals = linspace(-10, 10, 100);			%theta0范围
theta1_vals = linspace(-1, 4, 100);				%theta1范围
J_vals = zeros(length(theta0_vals), length(theta1_vals));	%初始化J为0阵

% 计算不同θ范围内，J的值
for i = 1:length(theta0_vals)
    for j = 1:length(theta1_vals)
	  t = [theta0_vals(i); theta1_vals(j)];
	  J_vals(i,j) = computeCost(X, y, t);
    end
end

J_vals = J_vals';

% 曲面图
subplot(1,2,1);
surf(theta0_vals, theta1_vals, J_vals);
xlabel('\theta_0'); ylabel('\theta_1');

% 轮廓图
subplot(1,2,2);
% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100
contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))
xlabel('\theta_0'); ylabel('\theta_1');
hold on;
plot(theta(1), theta(2), 'rx', 'MarkerSize', 10, 'LineWidth', 2);

end
```

